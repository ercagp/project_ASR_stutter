{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVYaXbcrQOaI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gqhDZeuPFQC",
        "outputId": "105e55bb-8d5b-4b63-8503-ae877bbf19b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-uospu8dc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-uospu8dc\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=e1bbfc9aeddffc2b8b8d03626335c6739512e7ecedf04719d51a0fca28a6b9a2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j32efm4i/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.0)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.1\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Aby9TyLrQPhv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import torchaudio\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knbzBOVi2y6H",
        "outputId": "189078b3-5630-4f2a-edcd-f01ee763c3a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 225MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model = whisper.load_model(\"base.en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "udOJCx6l0QQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65aa024c-8270-4665-9954-d13567adcfc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Can you tell us your name? My Caputo. And Mike, when was your stroke? I was seven years ago. Okay. And what did you used to do? Well, worked on a desk, seven sales, sales and worldwide and very good. Yeah. Okay. And who are you looking at over there? When you turn your head? That's my wife. Okay. And why is she helping you to talk? She's a speech. So you have trouble with your speech. Yeah. And what's that called? Phasia. All right. And so why don't you work now? I, well, I do. And what do you do now? Voices of Hope of Phasia. And what is Voices of Hope? Peterberg. Peterberg. And Dr. Hankley and myself, founder, founder for me. And I, I, members, members, the, the, the members, probably, seven, six, zero people. So 60 people are part of Voices of Hope, which is an aphasia support group that you founded. Yes. And Dr. Hankley is part of that. Yes. Okay. All right. It's not a support group. No, it's programs. It's, it's three month, three days. Um, um, um, Monday, Wednesday, Friday. And the, the, um, and they, and they, and they talked, um, music here. This is beautiful. It's, you know, great. Yeah. Can you tell me, um, what does it feel like to have aphasia? Um, it's, it's hard. It's, um, well, it's speech. It's like, um, words that don't understand. Brain is good, you know. It's, um, speech like, um, I don't know. It's like, um, words, yuck. Okay. Yeah. All right. Thank you so much. Bye bye. Bye bye.\n"
          ]
        }
      ],
      "source": [
        "result = model.transcribe(\"Broca's Aphasia (Non-Fluent Aphasia).mp3\")\n",
        "print(result[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = model.transcribe(\"Fluent Aphasia (Wernicke's Aphasia).mp3\")\n",
        "print(result2[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAR8OBDM8FeS",
        "outputId": "d8b0c23a-9c14-49d3-f4b3-0ca45bc7062c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hi, Byron. How are you? I'm happy. Are you pretty? You look good. What are you doing today? We stayed with the water over here at the moment and talked with the people over there. They're diving for them at the moment. They'll save in the moment. He held water first for him with luck for him. So we're on a cruise and we're about to do that. We will sort right here and they'll save their hands right there for them. And what were we just doing with the iPad? Uh, right at the moment they don't show a darn thing. The iPad that we were doing. We like here. I'd like my change for me and change hands for me. It was happy. I would talk with Donna sometimes. We're out with them. Other people are working with them with them. I'm very happy with them. This girl was really good and happy and I played golf and hit other trees. We play out with hands. We save a lot of hands on hold for people for us, other hands. I don't know what you get, but I talk with a lot of hands for him sometime. Am I talking of any more to say all right. Thank you very much. Thank you very much. I appreciate it. And I hope the world lasts for you. Thank you. It's been a pleasure. Bye bye. Have a good day.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ground truth:\n",
        "Hi Byron. How are you? I'm happy. Are you pretty? You look good. What are you doing today? We stayed with the water over here at the moment and <u>**talk**</u> with the people for them over there. They're diving for them at the moment, <u>**but**</u> they'll save in the moment held water very soon, for him, with luck, for him. So we're on a cruise and we're about to <u>**get to Juneau**</u>. We will sort right here and they'll save their hands right there for them. And what were we just doing with the iPad? Uh, right at the moment they don't show a darn thing. With the iPad that we were doing, like here? I'd like my change for me and change hands for me. It <u>**would**</u> happy. I would talk with Donna sometimes. We're out with them. Other people are working with them <u>**and**</u> them. I'm very happy with them. This girl <u>**with verly**</u> good. And happy and I played golf and hit other trees. We play out with <u>**the hands**</u>. We save a lot of hands on hold for <u>**peoples**</u>, for us. Other hands. I don't know what you get, but I talk with a lot of <u>**hand**</u> for him. Sometime. Am I <u>**talk**</u> of <u>**anymore**</u> to <u>**saying**</u>. <u>**Alright**</u>, thank you very much. Thank you very much, I appreciate it, and I hope the world lasts for you. Thank you, it's been a pleasure. Bye-bye. Have a good day.\n",
        "\n",
        "---\n",
        "\n",
        "Whisper's output:\n",
        "Hi, Byron. How are you? I'm happy. Are you pretty? You look good. What are you doing today? We stayed with the water over here at the moment and <u>**talked**</u> with the people over there. They're diving for them at the moment. They'll save in the moment. <u>**He**</u> held water first for him with luck for him. So we're on a cruise and we're about to <u>**do that**</u>. We will sort right here and they'll save their hands right there for them. And what were we just doing with the iPad? Uh, right at the moment they don't show a darn thing. The iPad that we were doing. <u>**We**</u> like here. I'd like my change for me and change hands for me. It <u>**was**</u> happy. I would talk with Donna sometimes. We're out with them. Other people are working with them <u>**with**</u> them. I'm very happy with them. This girl <u>**was really**</u> good and happy and I played golf and hit other trees. We play out with hands. We save a lot of hands on hold for <u>**people**</u> for us, other hands. I don't know what you get, but I talk with a lot of <u>**hands**</u> for him sometime. Am I <u>**talking**</u> of <u>**any more**</u> to <u>**say all right**</u>. Thank you very much. Thank you very much. I appreciate it. And I hope the world lasts for you. Thank you. It's been a pleasure. Bye bye. Have a good day.\n",
        "\n",
        "---\n",
        "\n",
        "(left punctuation alone because we will not deal with punctuation for now)\n",
        "Mistakes Whisper made: changed tense or quantity(talk > talked, peoples > people, hand > hands)\n",
        "\n",
        "Whisper dealt with fluency issues by using context clues to fix tense, quantity agreement issues.\n",
        "\n",
        "When both Megan (interviewer) and Byron (interviewee) were speaking at the same time (\"So we're on a cruise and we're about to get to Juneau\"), Whisper did not identify the correct words because two people were speaking at the same time.\n",
        "\n",
        "Whisper thought \"say all right\" but it really was \"saying. All right\" with a change in speaker.\n",
        "\n",
        "Whisper hallucinated \"with them with them\" for \"with them and them and \"We like here\" for \"like here?\""
      ],
      "metadata": {
        "id": "o-64yzvdNC8t"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}