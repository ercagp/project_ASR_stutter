{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVYaXbcrQOaI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gqhDZeuPFQC",
        "outputId": "53ebb9d1-d6cb-4be2-8f8d-494225ed234a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-oj_rvpv4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-oj_rvpv4\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=9a81322f2e548385f782ce531a264cedc66e9b5a6f0f706db97286f7fac71439\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qlfzqtq9/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.0)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.1\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aby9TyLrQPhv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import torchaudio\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knbzBOVi2y6H",
        "outputId": "6aec698f-8ab7-4f8f-9d3a-9385f63332d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:07<00:00, 64.4MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model = whisper.load_model(\"small.en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udOJCx6l0QQS",
        "outputId": "729b3edd-0a67-44f5-a2b1-4ad2b7c8c6a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Can you tell us your name? Mike Caputo. And Mike, when was your stroke? I was seven years ago. And what did you used to do? I well worked on an audidexque, sales worldwide, and very good. And who are you looking at over there? When you turn your head? That's my wife. Okay. And why is she helping you to talk? She's a speech. So you have trouble with your speech? Yeah. And what's that called? Phasia. All right. And so why don't you work now? I well I do. And what do you do now? Voices of hope of phasia. What is voices of hope? Peter Berg, Dr. Hinkley, and myself, founder for me. And I members, members, the members, probably seven, six, zero people. So 60 people are part of voices of hope, which is an aphasia support group that you founded. And Dr. Jackie Hinkley is part of that. Okay. Frank. It's not a support group. No. It's a program. It's three days. Three days. Monday. Wednesday. Friday. And they laugh and talked. Music here. It's just beautiful. It's, you know. Great. Yeah. Can you tell me, what does it feel like to have aphasia? It's hard. It's well, it's speech. It's like words that don't understand. Brain is good, you know. But it's speech like, I don't know, it's like words, yuck. Okay. Yeah. All right. Thank you so much. Good. Bye bye. Bye bye.\n"
          ]
        }
      ],
      "source": [
        "result = model.transcribe(\"brocas_aphasia_nonfluent.mp3\")\n",
        "print(result[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAR8OBDM8FeS",
        "outputId": "0d77a990-e9a5-4495-aed0-efdde21f7799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Hi, Byron, how are you? I'm happy. Are you pretty? You look good. What are you doing today? We stayed with the water over here at the moment and talked with the people for them over there. They're diving for them at the moment. They'll save in the moment. He held water very soon for him with luck for him. So we're on a cruise and we're about to get to the funeral. We will sort right here and they'll save their hands right there for them. What were we just doing with the iPad? Right at the moment. They don't show a darn thing. The iPad that we were doing? Like here? It might change for me and change hands for me. It was happy. I would talk with Donna sometimes. We're out with them. Other people are working with them. I'm very happy with them. This girl was very good and happy. I played golf and hit other trees. We play out with the hands. We save a lot of hands on hold for people, for us, other hands. I don't know what you get, but I talk with a lot of hands for him. Sometime. Am I talking of any more to say? Alright, thank you very much. Thank you very much. I appreciate it and I hope the world lasts for you. Thank you. It's been a pleasure. Bye bye. Have a good day.\n"
          ]
        }
      ],
      "source": [
        "result2 = model.transcribe(\"wernickes_aphasia_fluent.mp3\")\n",
        "print(result2[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-64yzvdNC8t"
      },
      "source": [
        "base.en\n",
        "---\n",
        "\n",
        "Ground truth:\n",
        "Hi Byron. How are you? I'm happy. Are you pretty? You look good. What are you doing today? We stayed with the water over here at the moment and <u>**talk**</u> with the people for them over there. They're diving for them at the moment, <u>**but**</u> they'll save in the moment held water very soon, for him, with luck, for him. So we're on a cruise and we're about to <u>**get to Juneau**</u>. We will sort right here and they'll save their hands right there for them. And what were we just doing with the iPad? Uh, right at the moment they don't show a darn thing. With the iPad that we were doing, like here? I'd like my change for me and change hands for me. It <u>**would**</u> happy. I would talk with Donna sometimes. We're out with them. Other people are working with them <u>**and**</u> them. I'm very happy with them. This girl <u>**with verly**</u> good. And happy and I played golf and hit other trees. We play out with <u>**the hands**</u>. We save a lot of hands on hold for <u>**peoples**</u>, for us. Other hands. I don't know what you get, but I talk with a lot of <u>**hand**</u> for him. Sometime. Am I <u>**talk**</u> of <u>**anymore**</u> to <u>**saying**</u>. <u>**Alright**</u>, thank you very much. Thank you very much, I appreciate it, and I hope the world lasts for you. Thank you, it's been a pleasure. Bye-bye. Have a good day.\n",
        "\n",
        "---\n",
        "\n",
        "Whisper's output:\n",
        "Hi, Byron. How are you? I'm happy. Are you pretty? You look good. What are you doing today? We stayed with the water over here at the moment and <u>**talked**</u> with the people over there. They're diving for them at the moment. They'll save in the moment. <u>**He**</u> held water first for him with luck for him. So we're on a cruise and we're about to <u>**do that**</u>. We will sort right here and they'll save their hands right there for them. And what were we just doing with the iPad? Uh, right at the moment they don't show a darn thing. The iPad that we were doing. <u>**We**</u> like here. I'd like my change for me and change hands for me. It <u>**was**</u> happy. I would talk with Donna sometimes. We're out with them. Other people are working with them <u>**with**</u> them. I'm very happy with them. This girl <u>**was really**</u> good and happy and I played golf and hit other trees. We play out with hands. We save a lot of hands on hold for <u>**people**</u> for us, other hands. I don't know what you get, but I talk with a lot of <u>**hands**</u> for him sometime. Am I <u>**talking**</u> of <u>**any more**</u> to <u>**say all right**</u>. Thank you very much. Thank you very much. I appreciate it. And I hope the world lasts for you. Thank you. It's been a pleasure. Bye bye. Have a good day.\n",
        "\n",
        "---\n",
        "\n",
        "(left punctuation alone because we will not deal with punctuation for now)\n",
        "Mistakes Whisper made: changed tense or quantity(talk > talked, peoples > people, hand > hands)\n",
        "\n",
        "Whisper dealt with fluency issues by using context clues to fix tense, quantity agreement issues.\n",
        "\n",
        "When both Megan (interviewer) and Byron (interviewee) were speaking at the same time (\"So we're on a cruise and we're about to get to Juneau\"), Whisper did not identify the correct words because two people were speaking at the same time.\n",
        "\n",
        "Whisper thought \"say all right\" but it really was \"saying. All right\" with a change in speaker.\n",
        "\n",
        "Whisper hallucinated \"with them with them\" for \"with them and them and \"We like here\" for \"like here?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyZcTe51fgPU"
      },
      "source": [
        "small.en\n",
        "---\n",
        "Ground truth:\n",
        "Hi Byron. How are you? I'm happy. Are you pretty? You look good. What are you doing today? We stayed with the water over here at the moment and talk with the people for them over there. They're diving for them at the moment, <u>but</u> they'll save in the moment held water very soon, for him, with luck, for him. So we're on a cruise and we're about to get to Juneau. We will sort right here and they'll save their hands right there for them. And what were we just doing with the iPad? <u>Uh</u>, right at the moment they don't show a darn thing. <u>With</u> the iPad that we were doing, like here? <u>I'd like my</u> change for me and change hands for me. It would happy. I would talk with Donna sometimes. We're out with them. Other people are working with them <u>and them</u>. I'm very happy with them. This girl with verly good. And happy <u>and</u> I played golf and hit other trees. We play out with the hands. We save a lot of hands on hold for peoples, for us. Other hands. I don't know what you get, but I talk with a lot of hand for him. Sometime. Am I talk of anymore to saying. Alright, thank you very much. Thank you very much, I appreciate it, and I hope the world lasts for you. Thank you, it's been a pleasure. Bye-bye. Have a good day.\n",
        "\n",
        "---\n",
        "Hi, Byron, how are you? I'm happy. Are you pretty? You look good. What are you doing today? We stayed with the water over here at the moment and <u>talked</u> with the people for them over there. They're diving for them at the moment. They'll save in the moment. <u>He</u> held water very soon for him with luck for him. So we're on a cruise and we're about to get to <u>**the funeral**</u>. We will sort right here and they'll save their hands right there for them. What were we just doing with the iPad? Right at the moment. They don't show a darn thing. The iPad that we were doing? Like here? It might change for me and change hands for me. It <u>was</u> happy. I would talk with Donna sometimes. We're out with them. Other people are working with them. I'm very happy with them. This girl <u>was very good</u> and happy. I played golf and hit other trees. We play out with the hands. We save a lot of hands on hold for people, for us, other hands. I don't know what you get, but I talk with a lot of <u>hands</u> for him. Sometime. Am I talking of <u>any more</u> to <u>say</u>? Alright, thank you very much. Thank you very much. I appreciate it and I hope the world lasts for you. Thank you. It's been a pleasure. Bye bye. Have a good day.\n",
        "\n",
        "---\n",
        "Many errors were the same as with base.en, but small.en tried to decode what both Megan and Byron were saying with they were both speaking, although it was still wrong. small.en tried to fix grammatical errors even further by replacing \"with them and them\" with \"with them\", for example. In this sense, when transcribing speech influenced by aphasia, this model may perform worse than the base model, which does not make so much effort to correct grammatical errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Broca's Case\n",
        "\n",
        "#### Ground truth\n",
        "Can you tell us your name? **I** Mike Caputo. And Mike, when was your stroke? I was hmm seven years ago. And, what did you used to do? I well worked **auto desk**, **seven seven**, sales **and** worldwide, and very good, **yeah**. And, who are you at over there? That's my wife. Okay. And, why is she helping you to talk? She's speech. So, you have trouble with your speech. Yeah, **yeah**. And, what's that called? Aphasia. All right. And, so why don't work now? I well, I do. And, what do you do now? Voices of hope of **aphasia**. And, what is voices of hope? Peter Berg, Dr. Hinkley, and, myself founder for me. And, I members, members, the members probably seven,six, zero people. So 60 people are part of voices of hope, which is an aphasia support group that you founded. And, Dr. Jackie Hinkley is **a** part of that. Okay. **Great**. It's not a support group. No. It's program**s**. It's three days. Monday, Wednesday, Friday. And they **laughed** and talked, music here it's just beautiful. Yeah. Great. Can you tell me what does it feel like to have aphasia? It's hard. **It's**, well, it's speech, it's like words that don't understand. Brain is good, you know. But, it's speech like, I don't know, it's like words, yuck. Okay. Yeah. All right. Thank you so much. Good. Bye bye. Bye bye.\n",
        "\n",
        "\n",
        "#### Model Output\n",
        "Can you tell us your name? Mike Caputo. And Mike, when was your stroke? I was seven years ago. And what did you used to do? I well worked on an audidexque, sales worldwide, and very good. And who are you looking at over there? When you turn your head? That's my wife. Okay. And why is she helping you to talk? She's a speech. So you have trouble with your speech? Yeah. And what's that called? Phasia. All right. And so why don't you work now? I well I do. And what do you do now? Voices of hope of phasia. What is voices of hope? Peter Berg, Dr. Hinkley, and myself, founder for me. And I members, members, the members, probably seven, six, zero people. So 60 people are part of voices of hope, which is an aphasia support group that you founded. And Dr. Jackie Hinkley is part of that. Okay. Frank. It's not a support group. No. It's a program. It's three days. Three days. Monday. Wednesday. Friday. And they laugh and talked. Music here. It's just beautiful. It's, you know. Great. Yeah. Can you tell me, what does it feel like to have aphasia? It's hard. It's well, it's speech. It's like words that don't understand. Brain is good, you know. But it's speech like, I don't know, it's like words, yuck. Okay. Yeah. All right. Thank you so much. Good. Bye bye. Bye bye."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
